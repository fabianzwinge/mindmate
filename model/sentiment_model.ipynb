{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18b55b6",
   "metadata": {},
   "source": [
    "## Prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416faea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers datasets torch scikit-learn huggingface_hub langdetect accelerate tf-keras\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea12a3",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef750869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds   = dataset[\"validation\"]\n",
    "test_ds  = dataset[\"test\"]\n",
    "\n",
    "#DOWNSAMPLE DATASET FOR FASTER EXPERIMENTATION\n",
    "# train_df = pd.DataFrame(dataset[\"train\"])\n",
    "# val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "# test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "# # Reduce size due to resource constraints\n",
    "# train_size = 20000\n",
    "# val_size   = 5000\n",
    "# test_size  = 5000\n",
    "\n",
    "# def reduce_sample(df, n, seed=42):\n",
    "#     return df.groupby(\"label\", group_keys=False).apply(\n",
    "#         lambda x: x.sample(frac=n/len(df), random_state=seed)\n",
    "#     ).reset_index(drop=True).sample(frac=1, random_state=seed)\n",
    "\n",
    "# def reduce_sample_balanced(df, n_per_class, seed=42):\n",
    "#     return pd.concat([\n",
    "#         df[df['label'] == l].sample(n=n_per_class//len(df['label'].unique()), random_state=seed)\n",
    "#         for l in df['label'].unique()\n",
    "#     ]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "#train_small = reduce_sample(train_df, train_size)\n",
    "#val_small   = reduce_sample(val_df, val_size)\n",
    "#test_small  = reduce_sample(test_df, test_size)\n",
    "\n",
    "# train_small = reduce_sample_balanced(train_df, train_size)\n",
    "# val_small   = reduce_sample_balanced(val_df, val_size)\n",
    "# test_small  = reduce_sample_balanced(test_df, test_size)\n",
    "\n",
    "# train_ds = Dataset.from_pandas(train_small,preserve_index=False)\n",
    "# val_ds   = Dataset.from_pandas(val_small,preserve_index=False)\n",
    "# test_ds  = Dataset.from_pandas(test_small,preserve_index=False)\n",
    "\n",
    "# dataset_reduced = DatasetDict({\n",
    "#     \"train\": train_ds,\n",
    "#     \"validation\": val_ds,\n",
    "#     \"test\": test_ds\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb5c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9536, 'text': 'Cooking microwave pizzas, yummy', 'label': 2, 'sentiment': 'positive'}\n",
      "---\n",
      "{'id': 6135, 'text': 'Any plans of allowing sub tasks to show up in the widget?', 'label': 1, 'sentiment': 'neutral'}\n",
      "---\n",
      "{'id': 17697, 'text': \" I love the humor, I just reworded it. Like saying 'group therapy' instead`a 'gang banging'. Keeps my moms off my back.   Hahaha\", 'label': 2, 'sentiment': 'positive'}\n",
      "---\n",
      "{'id': 14182, 'text': ' naw idk what ur talkin about', 'label': 1, 'sentiment': 'neutral'}\n",
      "---\n",
      "{'id': 17840, 'text': ' That sucks to hear. I hate days like that', 'label': 0, 'sentiment': 'negative'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1b9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "Counter({1: 11649, 2: 10478, 0: 9105})\n",
      "\n",
      "Validation class counts:\n",
      "Counter({1: 1928, 2: 1760, 0: 1517})\n",
      "\n",
      "Test class counts:\n",
      "Counter({1: 1930, 2: 1730, 0: 1546})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Train class counts:\")\n",
    "print(Counter(train_ds['label']))\n",
    "\n",
    "print(\"\\nValidation class counts:\")\n",
    "print(Counter(val_ds['label']))\n",
    "\n",
    "print(\"\\nTest class counts:\")\n",
    "print(Counter(test_ds['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1931308",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05dc139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31232/31232 [01:06<00:00, 472.61 examples/s]\n",
      "Filter: 100%|██████████| 31232/31232 [00:00<00:00, 171584.24 examples/s]\n",
      "Map: 100%|██████████| 5205/5205 [00:11<00:00, 465.12 examples/s]\n",
      "Filter: 100%|██████████| 5205/5205 [00:00<00:00, 140510.21 examples/s]\n",
      "Map: 100%|██████████| 5206/5206 [00:11<00:00, 462.15 examples/s]\n",
      "Filter: 100%|██████████| 5206/5206 [00:00<00:00, 116276.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "def reduce_lengthening(text: str) -> str:\n",
    "    # \"looooove\" -> \"loove\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "def clean_tweet(text: str, remove_hashtags=False) -> str:\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    if remove_hashtags:\n",
    "        text = re.sub(r\"#\\w+\", \" \", text)\n",
    "    text = re.sub(r\"[^0-9A-Za-z\\s\\.\\,\\!\\?\\:\\;\\-\\'\\\"]\", \" \", text)\n",
    "    text = reduce_lengthening(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = \"unknown\"\n",
    "\n",
    "    if lang != \"en\":\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_cleaning(ds: Dataset) -> Dataset:\n",
    "    ds = ds.map(lambda ex: {\"clean_text\": clean_tweet(ex[\"text\"])}, batched=False)\n",
    "    ds = ds.filter(lambda ex: ex[\"clean_text\"] != \"\")\n",
    "    return ds\n",
    "\n",
    "train_ds = apply_cleaning(train_ds)\n",
    "val_ds   = apply_cleaning(val_ds)\n",
    "test_ds  = apply_cleaning(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3460626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9536, 'text': 'Cooking microwave pizzas, yummy', 'label': 2, 'sentiment': 'positive', 'clean_text': 'cooking microwave pizzas, yummy'}\n",
      "---\n",
      "{'id': 6135, 'text': 'Any plans of allowing sub tasks to show up in the widget?', 'label': 1, 'sentiment': 'neutral', 'clean_text': 'any plans of allowing sub tasks to show up in the widget?'}\n",
      "---\n",
      "{'id': 17697, 'text': \" I love the humor, I just reworded it. Like saying 'group therapy' instead`a 'gang banging'. Keeps my moms off my back.   Hahaha\", 'label': 2, 'sentiment': 'positive', 'clean_text': \"i love the humor, i just reworded it. like saying 'group therapy' instead a 'gang banging'. keeps my moms off my back. hahaha\"}\n",
      "---\n",
      "{'id': 14182, 'text': ' naw idk what ur talkin about', 'label': 1, 'sentiment': 'neutral', 'clean_text': 'naw idk what ur talkin about'}\n",
      "---\n",
      "{'id': 17840, 'text': ' That sucks to hear. I hate days like that', 'label': 0, 'sentiment': 'negative', 'clean_text': 'that sucks to hear. i hate days like that'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfb052",
   "metadata": {},
   "source": [
    "## Tokenisation and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ecf23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28657/28657 [00:01<00:00, 18745.25 examples/s]\n",
      "Map: 100%|██████████| 4772/4772 [00:00<00:00, 20526.35 examples/s]\n",
      "Map: 100%|██████████| 4779/4779 [00:00<00:00, 22438.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 64 \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds   = val_ds.map(tokenize, batched=True)\n",
    "test_ds  = test_ds.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9467812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9536, 'text': 'Cooking microwave pizzas, yummy', 'label': 2, 'sentiment': 'positive', 'clean_text': 'cooking microwave pizzas, yummy', 'input_ids': [101, 8434, 18302, 10733, 2015, 1010, 9805, 18879, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "---\n",
      "{'id': 6135, 'text': 'Any plans of allowing sub tasks to show up in the widget?', 'label': 1, 'sentiment': 'neutral', 'clean_text': 'any plans of allowing sub tasks to show up in the widget?', 'input_ids': [101, 2151, 3488, 1997, 4352, 4942, 8518, 2000, 2265, 2039, 1999, 1996, 15536, 24291, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "---\n",
      "{'id': 17697, 'text': \" I love the humor, I just reworded it. Like saying 'group therapy' instead`a 'gang banging'. Keeps my moms off my back.   Hahaha\", 'label': 2, 'sentiment': 'positive', 'clean_text': \"i love the humor, i just reworded it. like saying 'group therapy' instead a 'gang banging'. keeps my moms off my back. hahaha\", 'input_ids': [101, 1045, 2293, 1996, 8562, 1010, 1045, 2074, 2128, 18351, 2098, 2009, 1012, 2066, 3038, 1005, 2177, 7242, 1005, 2612, 1037, 1005, 6080, 22255, 1005, 1012, 7906, 2026, 3566, 2015, 2125, 2026, 2067, 1012, 5292, 3270, 3270, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "---\n",
      "{'id': 14182, 'text': ' naw idk what ur talkin about', 'label': 1, 'sentiment': 'neutral', 'clean_text': 'naw idk what ur talkin about', 'input_ids': [101, 6583, 2860, 8909, 2243, 2054, 24471, 2831, 2378, 2055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "---\n",
      "{'id': 17840, 'text': ' That sucks to hear. I hate days like that', 'label': 0, 'sentiment': 'negative', 'clean_text': 'that sucks to hear. i hate days like that', 'input_ids': [101, 2008, 19237, 2000, 2963, 1012, 1045, 5223, 2420, 2066, 2008, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc969888",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45eeb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ba81a",
   "metadata": {},
   "source": [
    "## Trainer Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca177b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_model\",\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",            \n",
    "    learning_rate=2e-5,                \n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,                \n",
    "    weight_decay=0.01,                \n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,                \n",
    "    load_best_model_at_end=True,      \n",
    "    metric_for_best_model=\"f1\",      \n",
    "    greater_is_better=True,           \n",
    "    warmup_steps=500,                 \n",
    "    lr_scheduler_type=\"linear\"    \n",
    ")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5d050",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c271ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3584' max='17915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3584/17915 32:46 < 2:11:08, 1.82 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613400</td>\n",
       "      <td>0.615670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss']. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:3290\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3289\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3290\u001b[39m     metric_value = \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mKeyError\u001b[39m: 'eval_f1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: acc, \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: f1}\n\u001b[32m     11\u001b[39m trainer = Trainer(\n\u001b[32m     12\u001b[39m     model=model,\n\u001b[32m     13\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m2\u001b[39m)]\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2790\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2789\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2795\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2796\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:3222\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m   3221\u001b[39m     metrics = \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval)\n\u001b[32m-> \u001b[39m\u001b[32m3222\u001b[39m     is_new_best_metric = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_best_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n\u001b[32m   3225\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:3292\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3290\u001b[39m     metric_value = metrics[metric_to_check]\n\u001b[32m   3291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m-> \u001b[39m\u001b[32m3292\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   3293\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, which is not found in the evaluation metrics. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3294\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3295\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m   3297\u001b[39m operator = np.greater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.greater_is_better \u001b[38;5;28;01melse\u001b[39;00m np.less\n\u001b[32m   3299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: \"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss']. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d57b32",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3572ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.8097283244132996, 'test_runtime': 105.9566, 'test_samples_per_second': 64.696, 'test_steps_per_second': 8.088}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81      3432\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.82      0.79      0.81      3423\n",
      "\n",
      "    accuracy                           0.80      6855\n",
      "   macro avg       0.55      0.53      0.54      6855\n",
      "weighted avg       0.82      0.80      0.81      6855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\fzw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_output = trainer.predict(test_ds)\n",
    "print(test_output.metrics)\n",
    "\n",
    "preds = np.argmax(test_output.predictions, axis=1)\n",
    "labels = test_output.label_ids\n",
    "print(classification_report(labels, preds, labels=[0,1,2], target_names=[\"negative\",\"neutral\",\"positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001cddc",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06865c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer saved to ./sentiment_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAVE_DIR = \"./sentiment_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "trainer.save_model(SAVE_DIR)             \n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Model & tokenizer saved to\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6e9b3",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs = [\n",
    "    \"im feeling bad\",        # Negative sentiment\n",
    "    \"neutral\",               # Neutral sentiment\n",
    "    \"I absolutely love this\" # Positive sentiment\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**tokenized_inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "predictions = torch.argmax(logits, axis=1)\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "confidences = torch.max(probs, axis=1).values\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Probabilities:\\n\", probs)\n",
    "print(\"Confidences:\", confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bd414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee383b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
